Python을 사용하니까 그 기초를 좀 보자
    컴퓨터의 CPU, RAM, SSD를 활용하기 위해서
        1. 변수선언 : RAM 사용 문법 - 식별자(저장공간을 구분하는 문자열) - PEP20, PEP8(autopep8, flake)
        2. 데이터타입 : RAM 효율적 사용 문법 - int, float, bool, str, list, tuple, dict, set
        3. 연산자 : CPU 사용 문법 : 산술, 비교, 논리, 할당, 멤버 ...
        4. 조건문, 반복문 : if, elif, while, for, break, continue, range() ...
        5. 함수 : 반복 사용되는 코드를 묶어서 코드 작성 실행 : def, return, scope(global), lambda ...
        6. 클래스 : 변수, 함수를 묶어서 코드 작성 실행 - 객체지항 구현 - class, self, special methods(생성자)
        7. 모듈 : 변수, 함수, 클래스를 파일로 묶어서 코드 작성 실행 : 확장자 .py
        8. 패키지 : 여러개의 모듈을 디렉토리로 묶어서 관리: 버전정보 - import, from, as
        9. 입출력 : SSD 사용 문법 - RAM(object) > SSD(file), SSD(file) > RAM(object) - pickle
    
    class
        변수, 함수를 묶어서 코드 작성 실행
        객체지향 구현 : 실제세계를 모델링하여 프로그램을 개발하는 개발 방법론 : 헙업 용이
        함수 사용 : 함수 선언(코드 작성) > 함수 호출(코드 실행)
        클래스 사용
            클래스 선언(코드 작성-설계도 작성) > 객체 생성(메모리 사용-제품 생산) > 메서드 실행(코드 실행-제품 사용)
            메서드 : 클래스 안에 선언되는 함수
        class 식별자 : UpperCamelCase, PascalCase(O), snake_case(X:변수, 함수)


웹 크롤링
    웹 페이지에서 데이터를 수집하는 방법에 대해서 학습

    웹 페이지의 종류
        정적인 페이지 : 화면이 한번 뜨면 이벤트에 의한 화면 변경이 없음
        동적인 페이지 : 화면이 뜨고 이벤트가 발생하면 서버에서 데이터를 가져와 화면을 변경

    requests 이용 - 받아오는 문자열에 따라 2가지 방법으로 구분
        json 문자열로 받아서 파싱 : 주로 동적 페이지 크롤링할 때 사용
        html 문자열로 받아서 파싱 : 주로 정적 페이지 크롤링할 때 사용
    
    selenium 이용 - 브라우저를 직접 열어서 데이터를 받는 방법

    EX) 네이버 주식 데이터를 크롤링 해보자
        절차 : 웹 서비스 분석 - url : chrome devtool
            >> 서버에 데이터 요청 : request(url) - response : json(str)
            >> 서버에서 받은 데이터 파싱(형태 변경) - json(str) - list, dict - DataFrame

            0. 먼저 requests, pandas 를 import 한다
                import requests / import pandas as pd

            1. 크롤링하려는 url을 가져온다
                url = 'https://m.stock.naver.com/api/index/KOSPI/price?pageSize=60&page=1'

            2. 서버에 데이터를 요청한다 >> json(str) 형태로 나온다
                response = requests.get(url)
                response
                이때 response 로 200이 나오는지 확인해야 한다.
                403이나 500이라면 request가 잘못되었거나 웹 서버에서 수집이 안 되도록 설정된 것 - 이러면 header 설정 혹은 selenium 사용

            3. 서버에서 받은 데이터 파싱(형태 변경) >> 위의 json(str) 형태가 list, dict 로, 여기서 다시 DataFrame으로 변형
                data = response.json()
                df = DataFrame(data)
                df = df[['localTradedAt','closePrice']] // 여기에 들어간 column들은 response.text 로 알 수 있다 
                df.head(5) // 확인용

            4. 함수화
                def stock_price(code='KOSPI', page=1, page_size=60):
                    url = f'https://m.stock.naver.com/api/index/{code}/price?pageSize={page_size}&page={page}'
                    response = requests.get(url)
                    data = response.json()
                    return pd.DataFrame(data)[['localTradedAt', 'closePrice']]
                이걸 함수로 정의해두고
                stock_price('KOSDAQ', 3, 5) 처럼 사용할 수 있다

            5. 다른 주제(환율) 실습
                먼저 URL 가져오고
                page_size, page = 60, 1
                url = f'https://m.stock.naver.com/front-api/marketIndex/prices?category=exchange&reutersCode=FX_USDKRW&page={page}&pageSize={page_size}'
                
                데이터 요청하고
                response = requests.get(url)
                
                파싱 하고
                data = response.json()['result']
                df = pd.DataFrame(data)[['localTradedAt', 'closePrice']]
                df.tail(2)

                함수화
                def exchange_rate(code='FX_USDKRW', page=1, page_size=60):
                    url = f'https://m.stock.naver.com/front-api/marketIndex/prices?category=exchange&reutersCode={code}&page={page}&pageSize={page_size}'
                    response = requests.get(url)
                    data = response.json()['result']
                    return pd.DataFrame(data)[['localTradedAt', 'closePrice']]

            6. 시각화
                위에서 만든 함수들을 이용해 kospi, kosdaq, 환율의 데이터를 수집하자
                kp_df = stock_price('KOSPI', page_size=page_size)
                kd_df = stock_price('KOSDAQ', page_size=page_size)
                usd_df = exchange_rate(page_size=page_size)

                데이터 전처리 - 데이터 타입을 string 에서 float 로 변경
                kp_df['closePrice'] = kp_df['closePrice'].apply(lambda data: float(data.replace(',', '')))
                kd_df['closePrice'] = kd_df['closePrice'].apply(lambda data: float(data.replace(',', '')))
                usd_df['closePrice'] = usd_df['closePrice'].apply(lambda data: float(data.replace(',', '')))

                데이터 전처리 - merge로 날짜 데이터 맞추기
                merge_df = pd.merge(kp_df, kd_df, on='localTradedAt') >> 코스피랑 코스닥 맞추고
                merge_df = pd.merge(merge_df, usd_df, on='localTradedAt') >> 그걸 다시 환율이랑 맞춘 다음
                merge_df.columns = ['date', 'kospi', 'kosdaq', 'usd'] >> 열이름 재정의. 이건 필수는 아니지만 하면 보기 편하다

                시각화를 위해 matplotlib import
                import matplotlib.pyplot as plt

                그래프 생성
                plt.figure(figsize=(20, 3))
                plt.plot(kp_df['localTradedAt'], kp_df['closePrice'], label='kospi')
                plt.plot(kd_df['localTradedAt'], kd_df['closePrice'], label='kosdaq')
                plt.plot(usd_df['localTradedAt'], usd_df['closePrice'], label='usd')
                plt.xticks(kp_df['localTradedAt'][::5])
                plt.legend()
                plt.show()

            7. 데이터 스케일링 >> 이게 뭐지?
                데이터를 일정한 범위 내로 변환하여, 머신러닝 알고리즘이 데이터를 더 효과적으로 학습할 수 있도록 돕는 전처리 과정
                여기서는 min max scaling으로

                먼저 sklearn을 import
                from sklearn.preprocessing import minmax_scale

                그리고 scaling
                minmax_scale(kp_df['closePrice'])

                다시 시각화
                plt.figure(figsize=(20, 3))
                plt.plot(kp_df['localTradedAt'], minmax_scale(kp_df['closePrice']), label='kospi')
                plt.plot(kd_df['localTradedAt'], minmax_scale(kd_df['closePrice']), label='kosdaq')
                plt.plot(usd_df['localTradedAt'], minmax_scale(usd_df['closePrice']), label='usd')
                plt.xticks(kp_df['localTradedAt'][::5])
                plt.legend()
                plt.show()

            8. 상관관계 분석
                피어슨 상관계수 - 두 데이터 집합 간의 상관도를 분석할 때 사용
                -1에 가까우면 서로 반대 방향으로, 1에 가까우면 서로 같은 방향으로, 0에 가까우면 서로 무관

                merge_df.iloc[:, 1:].corr() >> 분석하려는 열을 추출해(iloc) 상관관계를 계산(corr)